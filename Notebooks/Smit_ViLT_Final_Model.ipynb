{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtDfdYp6Y2-G"
      },
      "outputs": [],
      "source": [
        "# Installing and Uninstalling Required Packages\n",
        "!pip install transformers datasets accelerate evaluate gradio torchvision torch matplotlib\n",
        "!pip uninstall openai -y\n",
        "!pip install openai==0.28.0\n",
        "\n",
        "# Importing Libraries and Packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import ViltForQuestionAnswering, ViltProcessor\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.amp import autocast, GradScaler\n",
        "from transformers import AdamW, get_scheduler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from evaluate import load\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import models, transforms\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Loading Dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/SkinCAP/Final Complete Dataset.csv')\n",
        "df['image_path'] = df['skincap_file_path'].apply(lambda x: os.path.join('/content/drive/MyDrive/SkinCAP/skincap', x))\n",
        "\n",
        "# Function to Load Image\n",
        "def load_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = image.resize((384, 384))\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Creation and Preprocessing**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3xgSK7dcZMhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining SkinCap Dataset Class\n",
        "class SkinCapDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataframe, processor):\n",
        "        self.data = dataframe\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.data.iloc[idx]['image_path']\n",
        "        image = load_image(img_name)\n",
        "        question = self.data.iloc[idx]['question']\n",
        "        answer = self.data.iloc[idx]['answer']\n",
        "        encoding = self.processor(image, question, return_tensors=\"pt\")\n",
        "        return encoding, answer\n",
        "\n",
        "# Initialize Processor and Dataset\n",
        "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
        "dataset = SkinCapDataset(df, processor)\n",
        "\n",
        "# Function for Batch Collation\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = [item[0][\"input_ids\"].squeeze(0) for item in batch]\n",
        "    pixel_values = [item[0][\"pixel_values\"].squeeze(0) for item in batch]\n",
        "    attention_mask = [item[0][\"attention_mask\"].squeeze(0) for item in batch]\n",
        "    answers = [item[1] for item in batch]\n",
        "    input_ids_padded = pad_sequence(input_ids, batch_first=True)\n",
        "    attention_mask_padded = pad_sequence(attention_mask, batch_first=True)\n",
        "    return {\"input_ids\": input_ids_padded, \"pixel_values\": torch.stack(pixel_values), \"attention_mask\": attention_mask_padded}, answers\n",
        "\n",
        "# Splitting Dataset into Training and Validation\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_data, val_data = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Dataloaders\n",
        "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=8, collate_fn=collate_fn)\n",
        "val_dataloader = DataLoader(val_data, batch_size=8, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "glDuY1uSZJjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Architecture with Dropout Layer**"
      ],
      "metadata": {
        "id": "Hise10C4ZW8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Model Class with Dropout Layer\n",
        "class ViltForQuestionAnsweringWithDropout(nn.Module):\n",
        "    def __init__(self, model_name, num_labels):\n",
        "        super(ViltForQuestionAnsweringWithDropout, self).__init__()\n",
        "        self.model = ViltForQuestionAnswering.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.classifier = nn.Linear(self.model.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, pixel_values, attention_mask):\n",
        "        outputs = self.model.vilt(input_ids=input_ids, pixel_values=pixel_values, attention_mask=attention_mask)\n",
        "        pooled_output = outputs[0][:, 0]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "X57wG8p9Zo63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Class Weight Calculation and Model Training Setup**"
      ],
      "metadata": {
        "id": "c2H75SQQZrBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Class Weights\n",
        "unique_answers = df['answer'].unique()\n",
        "answer_to_idx = {answer: idx for idx, answer in enumerate(unique_answers)}\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.array(list(answer_to_idx.values())), y=df['answer'].map(answer_to_idx).values)\n",
        "weights = torch.tensor(class_weights, dtype=torch.float).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "# Initializing Model and Moving to GPU\n",
        "model = ViltForQuestionAnsweringWithDropout(\"dandelin/vilt-b32-finetuned-vqa\", num_labels=len(df['answer'].unique()))\n",
        "model = model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "# Loss Function, Optimizer, and Learning Rate Scheduler\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "scaler = GradScaler()\n",
        "lr_scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=10*len(train_dataloader))"
      ],
      "metadata": {
        "id": "WTFs4Cl3ZuDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Training Loop**"
      ],
      "metadata": {
        "id": "ePMnugB8ZxNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "num_epochs = 10\n",
        "progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in train_dataloader:\n",
        "        encoding, answers = batch\n",
        "        input_ids = encoding['input_ids'].to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "        pixel_values = encoding['pixel_values'].to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "        attention_mask = encoding['attention_mask'].to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "        labels = torch.tensor([answer_to_idx[ans] for ans in answers], device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")).long()\n",
        "\n",
        "        with autocast(device_type='cuda'):\n",
        "            outputs = model(input_ids=input_ids, pixel_values=pixel_values, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_dataloader):.4f}\")"
      ],
      "metadata": {
        "id": "EEo3NAk-Zwbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Validation**"
      ],
      "metadata": {
        "id": "LluQUBAgZ2d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the Model\n",
        "metric = load(\"accuracy\")\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "for batch in val_dataloader:\n",
        "    encoding, answers = batch\n",
        "    input_ids = encoding['input_ids'].to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "    pixel_values = encoding['pixel_values'].to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "    attention_mask = encoding['attention_mask'].to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "    labels = torch.tensor([answer_to_idx[ans] for ans in answers], device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")).long()\n",
        "\n",
        "    outputs = model(input_ids=input_ids, pixel_values=pixel_values, attention_mask=attention_mask)\n",
        "    predicted = outputs.argmax(-1)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    total += labels.size(0)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "iQHdBnmfZ53z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction with Uncertainty **Estimation**"
      ],
      "metadata": {
        "id": "c23BERy5Z9b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Predict with Uncertainty\n",
        "def predict_with_uncertainty(image, question, num_samples=100):\n",
        "    image = image.convert(\"RGB\")\n",
        "    image = image.resize((384, 384))\n",
        "\n",
        "    model.train()\n",
        "    encoding = processor(image, question, return_tensors=\"pt\").to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "    logits_list = []\n",
        "    for _ in range(num_samples):\n",
        "        outputs = model(input_ids=encoding['input_ids'], pixel_values=encoding['pixel_values'], attention_mask=encoding['attention_mask'])\n",
        "        logits_list.append(outputs)\n",
        "\n",
        "    model.eval()\n",
        "    logits = torch.stack(logits_list)\n",
        "    mean_logits = logits.mean(dim=0)\n",
        "    std_logits = logits.std(dim=0)\n",
        "\n",
        "    predicted_answer_idx = mean_logits.argmax(-1).item()\n",
        "    uncertainty = std_logits.mean().item()\n",
        "\n",
        "    return unique_answers[predicted_answer_idx], uncertainty"
      ],
      "metadata": {
        "id": "mfNzkiU6Z_K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Saving**"
      ],
      "metadata": {
        "id": "aC611kn7aEC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model Function\n",
        "def save_model(model, path=\"/content/drive/MyDrive/SkinCAP/vilt_skincap_model.pth\"):\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Model saved at {path}\")\n",
        "\n",
        "save_model(model)"
      ],
      "metadata": {
        "id": "tTiI7NZWaFbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Integration with OpenAI for Detailed Descriptions**"
      ],
      "metadata": {
        "id": "fb37DJoqaH68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import OpenAI\n",
        "import openai\n",
        "import gradio as gr\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "# Fetching Detailed Explanation from OpenAI\n",
        "def get_detailed_answer(predicted_answer):\n",
        "    prompt = f\"Please provide a vivid and detailed explanation about the skin condition '{predicted_answer}'.\"\n",
        "\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful medical assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=400,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        return response['choices'][0]['message']['content'].strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\""
      ],
      "metadata": {
        "id": "CpPkqmEyaHez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gradio Interface**"
      ],
      "metadata": {
        "id": "k_Hj6XyQaP-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio Interface for Skintelligence\n",
        "\n",
        "def predict(image, question):\n",
        "    predicted_answer, uncertainty = predict_with_uncertainty(image, question)\n",
        "\n",
        "    if uncertainty > 1.5:\n",
        "        return f\"Skintelligence Predicted Answer: {predicted_answer}, but the model is uncertain.\", \"\"\n",
        "\n",
        "    detailed_answer = get_detailed_answer(predicted_answer)\n",
        "\n",
        "    return f\"Skintelligence Predicted Answer: {predicted_answer}\", f\"Vivid Description: {detailed_answer}\"\n",
        "\n",
        "# CSS Styling for Gradio Interface\n",
        "css = \"\"\"\n",
        "body {\n",
        "    background: linear-gradient(to right, #1a1f36, #283c86);\n",
        "    color: #ffffff;\n",
        "    font-family: 'Roboto', sans-serif;\n",
        "}\n",
        "\n",
        ".gradio-container {\n",
        "    background-color: rgba(255, 255, 255, 0.1);\n",
        "    border-radius: 15px;\n",
        "    padding: 30px;\n",
        "    box-shadow: 0 4px 20px rgba(0, 0, 0, 0.3);\n",
        "}\n",
        "\n",
        ".gradio-title {\n",
        "    font-family: 'Poppins', sans-serif;\n",
        "    font-size: 4em;\n",
        "    text-align: center;\n",
        "    color: #00acc1;\n",
        "    margin-top: 100px;\n",
        "}\n",
        "\n",
        ".gradio-description {\n",
        "    font-family: 'Lato', sans-serif;\n",
        "    font-size: 1.5em;\n",
        "    text-align: center;\n",
        "    margin-bottom: 50px;\n",
        "    color: #cfd8dc;\n",
        "    max-width: 900px;\n",
        "    margin-left: auto;\n",
        "    margin-right: auto;\n",
        "}\n",
        "\n",
        ".gradio-inputs, .gradio-outputs {\n",
        "    margin-top: 30px;\n",
        "    border-top: 2px solid #00acc1;\n",
        "    padding-top: 20px;\n",
        "}\n",
        "\n",
        ".gradio-button {\n",
        "    background-color: #00acc1;\n",
        "    color: #ffffff;\n",
        "    font-size: 1.3em;\n",
        "    padding: 12px 30px;\n",
        "    border-radius: 8px;\n",
        "    transition: box-shadow 0.3s ease;\n",
        "    display: block;\n",
        "    margin: 40px auto;\n",
        "}\n",
        "\n",
        ".gradio-button:hover {\n",
        "    box-shadow: 0 0 20px rgba(0, 172, 193, 0.8);\n",
        "}\n",
        "\n",
        ".gradio-clear-button {\n",
        "    background-color: #ff6f61;\n",
        "    color: #ffffff;\n",
        "    font-size: 1.1em;\n",
        "    padding: 10px 25px;\n",
        "    border-radius: 8px;\n",
        "    transition: box-shadow 0.3s ease;\n",
        "    display: block;\n",
        "    margin: 20px auto;\n",
        "}\n",
        "\n",
        ".gradio-clear-button:hover {\n",
        "    box-shadow: 0 0 20px rgba(255, 111, 97, 0.8);\n",
        "}\n",
        "\n",
        ".gradio-image-box {\n",
        "    border: 2px solid #00acc1;\n",
        "    border-radius: 12px;\n",
        "    transition: border-color 0.3s ease;\n",
        "}\n",
        "\n",
        ".gradio-image-box:hover {\n",
        "    border-color: #00acc1;\n",
        "}\n",
        "\n",
        ".gradio-textbox {\n",
        "    font-size: 1.1em;\n",
        "    padding: 15px;\n",
        "    background-color: #283c86;\n",
        "    border-radius: 10px;\n",
        "    color: #ffffff;\n",
        "    border: 1px solid #00acc1;\n",
        "}\n",
        "\n",
        ".gradio-outputs textarea {\n",
        "    font-size: 1.2em;\n",
        "    line-height: 1.6;\n",
        "    background-color: #1a1f36;\n",
        "    color: #ffffff;\n",
        "    border: 1px solid #00acc1;\n",
        "    padding: 20px;\n",
        "    border-radius: 10px;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Launching Gradio Interface\n",
        "with gr.Blocks(css=css) as demo:\n",
        "\n",
        "    with gr.Column():\n",
        "        gr.Markdown(\"\"\"\n",
        "        <div style=\"text-align: center;\">\n",
        "            <h1 style=\"font-size: 5em; color: #00acc1; font-family: 'Poppins', sans-serif;\">\n",
        "                Welcome to Skintelligence\n",
        "            </h1>\n",
        "            <p style=\"font-size: 1.8em; color: #cfd8dc; font-family: 'Roboto', sans-serif; max-width: 900px; margin: 0 auto;\">\n",
        "                The future of dermatology is here! Upload an image of any skin condition, ask your question, and let our cutting-edge AI analyze and provide a smart, intuitive diagnosis with a vivid explanation. Revolutionizing skin health, one scan at a time.\n",
        "            </p>\n",
        "        </div>\n",
        "        \"\"\")\n",
        "        start_button = gr.Button(\"Start Your Diagnosis\")\n",
        "\n",
        "    with gr.Row(visible=False) as interface_row:\n",
        "        with gr.Column():\n",
        "            image_input = gr.Image(type=\"pil\", label=\"Upload Skin Image\", elem_id=\"gradio-image-box\")\n",
        "            question_input = gr.Textbox(lines=2, placeholder=\"Ask a question about the skin condition\", label=\"Your Question\", elem_id=\"gradio-textbox\")\n",
        "            submit_button = gr.Button(\"Get Diagnosis\", elem_id=\"gradio-button\")\n",
        "            clear_button = gr.Button(\"Clear\", elem_id=\"gradio-clear-button\")\n",
        "\n",
        "        with gr.Column():\n",
        "            output_predicted = gr.Textbox(label=\"Skintelligence Predicted Answer\", elem_id=\"gradio-outputs\")\n",
        "            output_vivid = gr.Textbox(label=\"Vivid Description\", elem_id=\"gradio-outputs\")\n",
        "\n",
        "        submit_button.click(predict, inputs=[image_input, question_input], outputs=[output_predicted, output_vivid])\n",
        "\n",
        "        clear_button.click(lambda: (None, \"\", \"\", \"\"), inputs=[], outputs=[image_input, question_input, output_predicted, output_vivid])\n",
        "\n",
        "    start_button.click(lambda: gr.update(visible=False), outputs=[start_button])\n",
        "    start_button.click(lambda: gr.update(visible=True), outputs=[interface_row])\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "ndk0AfSwaRGQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}