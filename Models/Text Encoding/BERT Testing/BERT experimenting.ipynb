{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel, TFBertForSequenceClassification, TFBertForMaskedLM\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv(\"/Users/samknowles/Downloads/skincap_vqa.csv\")\n",
    "df = pd.read_csv(\"hf://datasets/Mreeb/Dermatology-Question-Answer-Dataset-For-Fine-Tuning/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b0476e5ab334c76b044702e02e19e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "qa_pairs = [(df.loc[i,\"prompt\"], df.loc[i,\"response\"]) for i in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "730/730 [==============================] - 1043s 1s/step - loss: 0.2156\n",
      "Epoch 2/5\n",
      "730/730 [==============================] - 1083s 1s/step - loss: 0.0838\n",
      "Epoch 3/5\n",
      "730/730 [==============================] - 1120s 2s/step - loss: 0.0457\n",
      "Epoch 4/5\n",
      "730/730 [==============================] - 1138s 2s/step - loss: 0.0293\n",
      "Epoch 5/5\n",
      "730/730 [==============================] - 1182s 2s/step - loss: 0.0193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x315493ad0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_qa_data(qa_pairs):\n",
    "\n",
    "    texts = [f\"Question: {q} Answer: {a}\" for q, a in qa_pairs]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def mask_tokens(inputs, tokenizer):\n",
    "\n",
    "    inputs['labels'] = inputs['input_ids']\n",
    "\n",
    "    probability_matrix = np.random.rand(*inputs['input_ids'].shape)\n",
    "\n",
    "    special_tokens_mask = [\n",
    "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in inputs['input_ids'].numpy()\n",
    "    ]\n",
    "    probability_matrix = np.where(np.array(special_tokens_mask, dtype=bool), 0.0, probability_matrix)\n",
    "\n",
    "    masked_indices = probability_matrix < 0.15\n",
    "    inputs['input_ids'] = np.where(masked_indices, tokenizer.mask_token_id, inputs['input_ids'])\n",
    "\n",
    "    inputs['input_ids'] = tf.convert_to_tensor(inputs['input_ids'])\n",
    "    inputs['labelss'] = tf.convert_to_tensor(inputs['labels'])\n",
    "\n",
    "\n",
    "    return inputs\n",
    "\n",
    "inputs = prepare_qa_data(qa_pairs)\n",
    "\n",
    "masked_inputs = mask_tokens(inputs, tokenizer)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 5e-5)\n",
    "\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "model.fit(masked_inputs['input_ids'], masked_inputs['labels'], epochs=5, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(243,), dtype=int32, numpy=\n",
       "array([  103,  3160,  1024,  2054,   103,   103, 11069,  6190,   103,\n",
       "        2054,  2024,  2049,  2691,  8030,  1029,   103,  1024,  8827,\n",
       "       11069,  6190,  2003,  1037, 11888,  8285,  5714,   103,  2063,\n",
       "        4650,  2008,  3463,  1999,  1996,  2058, 21572, 16256,  1997,\n",
       "         103,  4442,  1012,  2023,  2058, 21572, 16256,  5260,  2000,\n",
       "         103,  1997,  4317,  1010,  2417,  3096,  3139,  2007, 21666,\n",
       "        9539,  1012,   103,  8030,  2421,  2417, 13864,  1997,  3096,\n",
       "        3139,  2007,  4317,  1010, 21666,  9539,  1010,  2235, 25169,\n",
       "        7516,   103,  4141,  2464,  1999,  2336,  1007,   103,  4318,\n",
       "        1998,  9630,  3096,  2008,  2089, 19501,   103,  2009,  8450,\n",
       "        1010,  5255,  1010,  2030, 14699,   103,  1010,  4317,  6675,\n",
       "        1010, 25895,  1010,   103,  5526,  2094, 10063,  1010,  1998,\n",
       "       13408,  1998, 10551, 17651,  1012,   103,   103,   103,   103,\n",
       "         103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
       "         103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
       "         103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
       "         103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
       "         103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
       "         103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
       "         103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
       "         103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
       "         103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
       "         103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
       "         103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
       "         103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
       "         103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
       "         103,   103,   103,   103,   103,   103,   103,   103,   103],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_inputs['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/samknowles/Desktop/Projects/iLab/Skintelligence/Notebooks'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"./my_pretrained_bert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./my_pretrained_bert/tokenizer_config.json',\n",
       " './my_pretrained_bert/special_tokens_map.json',\n",
       " './my_pretrained_bert/vocab.txt',\n",
       " './my_pretrained_bert/added_tokens.json')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
